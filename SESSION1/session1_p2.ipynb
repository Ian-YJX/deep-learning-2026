{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CmhjcRYR8d7O"
      },
      "source": [
        "# üß† Understanding Model Evaluation and the Bias‚ÄìVariance Tradeoff\n",
        "<a href=\"https://colab.research.google.com/github/ntu-dl-bootcamp/deep-learning-2026/blob/main/SESSION1/session1_p2.ipynb\" target=\"_blank\"><img alt=\"Open In Colab\" src=\"https://colab.research.google.com/assets/colab-badge.svg\"/></a>\n",
        "\n",
        "Welcome to this beginner-friendly, hands-on tutorial!  \n",
        "In this notebook, we‚Äôll walk through how to **evaluate and improve a machine learning model** step by step ‚Äî  \n",
        "using the **California Housing dataset** as a running example.\n",
        "\n",
        "Our aim is to make model evaluation *intuitive, visual, and practical* ‚Äî  \n",
        "so by the end, you‚Äôll not only know how to train models, but also how to think about their performance.\n",
        "\n",
        "---\n",
        "\n",
        "## üéØ What You‚Äôll Learn\n",
        "\n",
        "By the end of this session, you‚Äôll be able to:\n",
        "- üìä Prepare and explore data for an ML problem  \n",
        "- ‚úÇÔ∏è Split data correctly for training and testing  \n",
        "- üå≥ Train and interpret a Decision Tree model  \n",
        "- üìè Measure model performance using RMSE and R¬≤  \n",
        "- ‚öñÔ∏è Understand the bias‚Äìvariance tradeoff and tune model hyperparameters  \n",
        "\n",
        "> üí° These steps form a **blueprint for applied machine learning**.  Whether you work in materials science, healthcare, finance, or social sciences ‚Äî  you‚Äôll be able to use this same process to approach problems in your own field.\n",
        "\n",
        "---\n",
        "\n",
        "## üèåÔ∏è A Simple Analogy ‚Äî The ‚ÄúGolf Prediction‚Äù Example\n",
        "\n",
        "Imagine you‚Äôre trying to predict golf activity based on weather:\n",
        "- üå¶Ô∏è *Will someone play golf today or not?* ‚Üí **Classification problem**  \n",
        "- ‚õ≥ *How many hours will they play golf today?* ‚Üí **Regression problem**\n",
        "\n",
        "In this tutorial, we‚Äôll focus on the **regression case** ‚Äî  predicting a *continuous number* (i.e. predicting median house prices).\n",
        "\n",
        "---\n",
        "\n",
        "## üß≠ Step-by-Step Roadmap\n",
        "\n",
        "### 1Ô∏è‚É£ Preparing and Inspecting the Data\n",
        "- Load the dataset and understand what each feature represents.  \n",
        "- Visualize distributions and feature correlations.  \n",
        "- (Optional) Try one-hot encoding and feature scaling to see how preprocessing works.\n",
        "\n",
        "### 2Ô∏è‚É£ Splitting the Data (Training vs Testing)\n",
        "- Divide data into **training** and **testing** sets.  \n",
        "- Learn why we need separate data for evaluation ‚Äî so we can measure generalization.  \n",
        "\n",
        "### 3Ô∏è‚É£ Picking a Model ‚Äî Decision Tree\n",
        "- Introduce **Decision Trees** as intuitive, rule-based models.  \n",
        "- Train your first model using scikit-learn‚Äôs `DecisionTreeRegressor`.  \n",
        "- Understand what model parameters and hyperparameters mean.\n",
        "\n",
        "### 4Ô∏è‚É£ Evaluating Model Performance\n",
        "- Use **RMSE** to measure average prediction error.  \n",
        "- Use **R¬≤** to measure how well predictions follow the true trend.  \n",
        "- Compare training and test metrics to identify how well the model generalizes.\n",
        "\n",
        "### 5Ô∏è‚É£ Hyperparameters and Fair Tuning\n",
        "- Experiment with `max_depth` to see how model complexity affects results.  \n",
        "- Revisit **data splitting** ‚Äî introduce the **validation set** to tune hyperparameters fairly (without touching the test set).  \n",
        "- Observe how performance changes across depths and discuss which seems best.\n",
        "\n",
        "### 6Ô∏è‚É£ Bias‚ÄìVariance Tradeoff ‚Äî Understanding Model Complexity\n",
        "- Visualize how training and validation errors change as tree depth increases.  \n",
        "- Understand:\n",
        "  - **Underfitting (High Bias):** model too simple to learn patterns.  \n",
        "  - **Overfitting (High Variance):** model too complex, memorizes noise.  \n",
        "- Identify the ‚Äúsweet spot‚Äù where the model performs best on new data.\n",
        "\n",
        "### 7Ô∏è‚É£ Bonus Section ‚Äî Reducing Variance with Ensembles\n",
        "- Learn how **Random Forests** combine many decision trees to create a stronger, more stable model.  \n",
        "- Perform a small grid search to find the best Random Forest hyperparameters.  \n",
        "- Compare results with your tuned Decision Tree and discuss why ensembles often perform better.\n",
        "\n",
        "---\n",
        "\n",
        "## üåç Why This Matters\n",
        "\n",
        "This tutorial mirrors the **flow of any applied ML problem**:\n",
        "1. Define your prediction goal.  \n",
        "2. Explore and understand your dataset.  \n",
        "3. Train a model and measure its performance.  \n",
        "4. Tune hyperparameters and validate improvements.  \n",
        "5. Interpret results through the lens of bias‚Äìvariance.  \n",
        "6. Use ensemble methods to improve robustness.\n",
        "\n",
        "> üß© These ideas apply across all disciplines.  \n",
        "> If you can frame a question in terms of prediction and collect relevant data,  \n",
        "> this workflow will help you apply ML or DL methods systematically to your domain.\n",
        "\n",
        "---\n",
        "\n",
        "> ‚úÖ **In short:**  \n",
        "> This notebook is your *field guide* to evaluating, tuning, and understanding machine learning models ‚Äî  \n",
        "> from preparing data and training your first tree, to diagnosing bias‚Äìvariance behavior and improving performance with ensembles.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RoEHAparLYhY",
        "outputId": "54ad6617-9edf-42e1-e5fb-74319d61fd2a"
      },
      "outputs": [],
      "source": [
        "# =========================================================\n",
        "# üì¶ STEP 1: Import all the libraries we will use\n",
        "# =========================================================\n",
        "# Think of this as packing your toolbox before starting a project.\n",
        "\n",
        "# --- Core Python libraries ---\n",
        "import numpy as np                     # For handling numerical data\n",
        "import pandas as pd                    # For working with tabular (spreadsheet-like) data\n",
        "\n",
        "# --- Visualization libraries ---\n",
        "import matplotlib.pyplot as plt        # For plotting graphs\n",
        "import seaborn as sns                  # Makes plots look cleaner and more appealing\n",
        "\n",
        "# --- Machine Learning libraries from scikit-learn ---\n",
        "from sklearn.datasets import fetch_california_housing  # Built-in real-world dataset\n",
        "from sklearn.model_selection import train_test_split   # To split data into train/test sets\n",
        "from sklearn.tree import DecisionTreeRegressor         # Our first model (simple and interpretable)\n",
        "from sklearn.ensemble import RandomForestRegressor     # Ensemble model (many trees combined)\n",
        "from sklearn.metrics import mean_squared_error, r2_score  # For measuring performance\n",
        "\n",
        "# --- Optional: make the plots look consistent and nice ---\n",
        "sns.set(style=\"whitegrid\", palette=\"muted\", font_scale=1.2)\n",
        "\n",
        "# --- Optional: define a random seed for reproducibility ---\n",
        "RANDOM_SEED = 42\n",
        "np.random.seed(RANDOM_SEED)\n",
        "\n",
        "print(\"‚úÖ Libraries loaded successfully. Ready to begin!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m_9FuXG09psz"
      },
      "source": [
        "# üßæ Step 1: Preparing and Inspecting the Data\n",
        "\n",
        "Before we can train any model, we need to **load and understand our dataset**.\n",
        "\n",
        "Machine learning models don‚Äôt work directly on raw data ‚Äî they need clean, structured information.  \n",
        "So our first step is to **inspect what we‚Äôre working with**.\n",
        "\n",
        "In this tutorial, we‚Äôll use the **California Housing dataset**, which contains data about different neighborhoods in California:\n",
        "- üè° Each row represents one district (a small area)\n",
        "- üìà The features include things like median income, average rooms per house, and population\n",
        "- üéØ The target variable (`MedHouseVal`) is the **median house value** in that district\n",
        "\n",
        "Let‚Äôs load the dataset, check how many examples and features we have, and take a quick peek at the data.\n",
        "\n",
        "> **Tip:** Always start by exploring the dataset before jumping into modeling.  \n",
        "> This helps you notice missing values, incorrect data, or unusual patterns early on.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ors1QSFwTWNw",
        "outputId": "82a67d28-4fb3-43a7-a441-837403405aed"
      },
      "outputs": [],
      "source": [
        "# =========================================================\n",
        "# 1Ô∏è‚É£ STEP 1: Preparing and Inspecting the Data\n",
        "# =========================================================\n",
        "\n",
        "# The California Housing dataset is included with scikit-learn.\n",
        "# It‚Äôs a great starter dataset for regression problems (predicting continuous values).\n",
        "# Setting `as_frame=True` gives us a nice pandas DataFrame directly.\n",
        "cal = fetch_california_housing(as_frame=True)\n",
        "\n",
        "# Separate the features (inputs) and the target variable (output we want to predict)\n",
        "X = cal.data   # 'X' usually stands for the input variables (independent variables)\n",
        "y = cal.target # 'y' is the target variable (dependent variable ‚Äî what we want to predict)\n",
        "\n",
        "# Display the shape of the feature matrix\n",
        "# This tells us how many samples (rows) and features (columns) we have\n",
        "print(\"‚úÖ Dataset loaded successfully!\")\n",
        "print(f\"Feature matrix shape: {X.shape}\")\n",
        "print(f\"Number of samples: {X.shape[0]}\")\n",
        "print(f\"Number of features: {X.shape[1]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HjMA3o2T_SKS"
      },
      "source": [
        "## üîç Inspecting the Data\n",
        "\n",
        "Before we do any modeling, we need to **understand what our dataset looks like**.  \n",
        "This step is called **exploratory data inspection** ‚Äî it‚Äôs like reading the recipe before cooking.\n",
        "\n",
        "We‚Äôll start by printing a few rows from the dataset to get a quick feel for it.  \n",
        "This lets us answer some basic but important questions:\n",
        "- What kind of features do we have?  \n",
        "- Are they numbers, categories, or something else?  \n",
        "- Do the values look reasonable?\n",
        "\n",
        "By default, the `head()` function in pandas shows the **first 5 rows** of a DataFrame, but you can pass a number (like `head(10)`) to see more.\n",
        "\n",
        "We‚Äôll also print the **shape** of the dataset ‚Äî it tells us how many rows and columns there are:\n",
        "- `X.shape[0]` ‚Üí the number of **examples** (districts)\n",
        "- `X.shape[1]` ‚Üí the number of **features** (columns)\n",
        "\n",
        "Once you know what the data looks like, it‚Äôs much easier to decide how to clean or prepare it for a model.\n",
        "\n",
        "> üß≠ **Want to explore more?**  \n",
        "> You can visit the [California Housing dataset page](https://scikit-learn.org/stable/datasets/real_world.html#california-housing-dataset) on scikit-learn‚Äôs website for a full description of each feature.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        },
        "id": "WqkcwcK2_Vez",
        "outputId": "5566529f-ac9f-4d75-e3ec-ec92acbb0de0"
      },
      "outputs": [],
      "source": [
        "# =========================================================\n",
        "# üëÄ STEP 1.1: Inspect the dataset\n",
        "# =========================================================\n",
        "\n",
        "# The simplest and most common first step when exploring data\n",
        "# is to look at the first few rows. This helps us understand:\n",
        "# - what kind of features are present\n",
        "# - what the values look like\n",
        "# - whether there are any obvious issues (like missing data)\n",
        "\n",
        "print(\"\\nüìä Preview of the dataset:\")\n",
        "display(X.head())  # display() works nicely in notebooks for readable tables\n",
        "\n",
        "# If you want to see more than 5 rows, try X.head(10) or X.tail() to see the last few."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rCEeof2aB8mA",
        "outputId": "2672ab32-9cd8-4fc7-eeac-cbaf79546d41"
      },
      "outputs": [],
      "source": [
        "# ---------------------------------------------------------\n",
        "# Let's summarize what we‚Äôve learned so far\n",
        "# ---------------------------------------------------------\n",
        "print(\"\\nüìè Dataset dimensions:\")\n",
        "print(f\"Number of rows (examples): {X.shape[0]}\")\n",
        "print(f\"Number of columns (features): {X.shape[1]}\")\n",
        "\n",
        "# You can also check the column names (feature list)\n",
        "print(\"\\nüî§ Feature names:\")\n",
        "print(list(X.columns))\n",
        "\n",
        "# Optional: view summary statistics (mean, std, min, max) for numerical columns\n",
        "# This helps check the data range and potential outliers.\n",
        "print(\"\\nüìà Quick summary statistics:\")\n",
        "print(X.describe())\n",
        "\n",
        "# --- Beginner tips ---\n",
        "# ‚Ä¢ X.shape[0] gives you the total number of observations (districts)\n",
        "# ‚Ä¢ X.shape[1] gives you the total number of input features\n",
        "# ‚Ä¢ X.describe() is a quick way to check basic statistics for each feature\n",
        "# ‚Ä¢ X.columns gives you the names of all the features in your dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C8B-J-UzCv6o"
      },
      "source": [
        "## üîé Visualizing the Data\n",
        "\n",
        "Now that we‚Äôve loaded the California Housing dataset, let‚Äôs take a quick visual tour of it.  \n",
        "This step is called **Exploratory Data Analysis (EDA)** ‚Äî it helps us understand what the data *looks like* and *how the features relate to the target*.\n",
        "\n",
        "Here are the main things we‚Äôll check:\n",
        "\n",
        "1. **Distributions** ‚Äî Are the features normally distributed, skewed, or have outliers?  \n",
        "   We‚Äôll use **histograms** to get a sense of their shapes.\n",
        "\n",
        "2. **Relationships with the target variable** ‚Äî  \n",
        "   How do features like income or house age relate to **median house value**?  \n",
        "   We‚Äôll use **scatter plots** to spot visible trends or correlations.\n",
        "\n",
        "3. **Feature correlations** ‚Äî  \n",
        "   A **heatmap** shows how strongly each feature relates to others, and especially to the target variable.\n",
        "\n",
        "> üß† **Why this matters:**  \n",
        "> - Distributions tell us whether we need transformations or scaling.  \n",
        "> - Correlations tell us which features might be most useful for prediction.  \n",
        "> - Visuals help us *see* the data before jumping into modeling.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "buPBgFcsTcUX",
        "outputId": "d489f92c-f288-47de-f68a-a7fe60f7292c"
      },
      "outputs": [],
      "source": [
        "# =========================================================\n",
        "# üîé STEP 1.2: Exploratory Data Analysis (EDA)\n",
        "# =========================================================\n",
        "\n",
        "sns.set(style=\"whitegrid\", palette=\"muted\", font_scale=1.2)\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# (A) Histograms ‚Äî look at how each feature is distributed\n",
        "# ---------------------------------------------------------\n",
        "# This helps identify skewed data, outliers, or unusual ranges.\n",
        "plt.figure(figsize=(14, 10))\n",
        "X.hist(bins=30, color='skyblue', edgecolor='none', figsize=(14, 10))\n",
        "plt.suptitle(\"Feature Distributions\", fontsize=18, y=1.02)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Beginner notes:\n",
        "# - If the histogram is skewed (e.g., heavily right-tailed), you might later apply transformations (like log or sqrt).\n",
        "# - Features with wide ranges might benefit from scaling."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 917
        },
        "id": "L8sbbNjBEgs8",
        "outputId": "dda2de9d-19b3-4184-b9e8-9ecfdd3b4f8f"
      },
      "outputs": [],
      "source": [
        "# ---------------------------------------------------------\n",
        "# (B) Scatter plots ‚Äî check how some key features relate to the target\n",
        "# ---------------------------------------------------------\n",
        "cols = ['MedInc', 'HouseAge', 'AveRooms', 'AveOccup']\n",
        "\n",
        "# Create 2x2 subplots for cleaner visualization\n",
        "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
        "for ax, col in zip(axes.flatten(), cols):\n",
        "    sns.scatterplot(x=X[col], y=y, alpha=0.5, ax=ax, color='royalblue', s=30)\n",
        "    ax.set_title(f\"{col} vs Median House Value\", fontsize=13)\n",
        "    ax.set_xlabel(col)\n",
        "    ax.set_ylabel(\"Median House Value\")\n",
        "\n",
        "plt.tight_layout(pad=3)\n",
        "plt.show()\n",
        "\n",
        "# Beginner notes:\n",
        "# - If you see an upward trend, it suggests a positive correlation (as one increases, so does the other).\n",
        "# - For example, higher MedInc (median income) usually means higher house prices."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 793
        },
        "id": "UOpmkYgyEjLC",
        "outputId": "07a22b8a-f16b-4ad4-ca98-c306375b847c"
      },
      "outputs": [],
      "source": [
        "# ---------------------------------------------------------\n",
        "# (C) Correlation Heatmap ‚Äî summarize relationships between all variables\n",
        "# ---------------------------------------------------------\n",
        "# Combine features and target into one DataFrame\n",
        "corr = pd.concat([X, y.rename(\"MedHouseVal\")], axis=1).corr()\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(\n",
        "    corr,\n",
        "    annot=True, fmt=\".2f\",\n",
        "    cmap=\"coolwarm\", cbar=True, square=True,\n",
        "    linewidths=0.5, annot_kws={\"size\": 8}\n",
        ")\n",
        "plt.title(\"Feature Correlation Heatmap\", fontsize=16)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Beginner notes:\n",
        "# - Correlation values range from -1 to +1.\n",
        "#   +1 ‚Üí perfect positive relationship, -1 ‚Üí perfect negative relationship, 0 ‚Üí no relationship.\n",
        "# - Look for features that have high correlation with the target (MedHouseVal).\n",
        "#   Those will likely be important predictors.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bw3P5QxOFljz"
      },
      "source": [
        "---\n",
        "\n",
        "## üí¨ **Activity 1 ‚Äî Let‚Äôs Discuss!**\n",
        "\n",
        "Now that we‚Äôve visualized the data, let‚Äôs pause and think critically about what we‚Äôve seen.\n",
        "\n",
        "#### üß† Question 1: Which features are most correlated with the target?\n",
        "Take a look at the correlation heatmap and scatter plots you created.\n",
        "\n",
        "<details>\n",
        "<summary>üí° Show hints</summary>\n",
        "\n",
        "- Which feature shows the *strongest upward trend* with the target?  \n",
        "- Which correlations in the heatmap are closer to **+1** or **‚àí1**?  \n",
        "- Does the relationship look linear, or does it flatten after a point?  \n",
        "- *Hint:* `MedInc` (Median Income) often has the strongest positive link to house value.  \n",
        "\n",
        "</details>\n",
        "\n",
        "---\n",
        "\n",
        "#### üß© Question 2: Which features might be redundant or overlapping?\n",
        "Sometimes, two or more features capture similar information ‚Äî this is called **multicollinearity**. It‚Äôs like having two friends telling you the same story twice ‚Äî not very helpful to the model!\n",
        "\n",
        "<details>\n",
        "<summary>üí° Show hints</summary>\n",
        "\n",
        "- Do any features have very high correlation with **each other**?  \n",
        "- Why can that cause issues in some models (like linear regression)?  \n",
        "- How could you handle redundant features (drop one, combine them, or use PCA)?  \n",
        "- *Hint:* Try comparing `AveRooms`, `AveBedrms`, and `HouseAge`.  \n",
        "\n",
        "</details>\n",
        "\n",
        "---\n",
        "\n",
        "üó£Ô∏è **In-Class Idea:** Discuss your answers in small groups and compare which features each team found most important.\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QSEW2jMkA4zx"
      },
      "source": [
        "## üß© Optional: Encoding Categorical Features (One-Hot Encoding)\n",
        "\n",
        "Many ML models expect **numbers**, not words.  \n",
        "If a feature is **categorical** (e.g., animal *species*), we should convert it to numbers **without inventing a fake order**.\n",
        "\n",
        "- **Label encoding** maps each category to a number (cat‚Üí0, dog‚Üí1, ‚Ä¶).  \n",
        "  This can accidentally introduce an **order** the model may misinterpret.\n",
        "- **One-hot encoding** creates **one column per category** and fills with 0/1.  \n",
        "  This avoids the fake ‚Äúless-than/greater-than‚Äù relationship.\n",
        "\n",
        "We‚Äôll use a tiny example to see:\n",
        "- A `species` column (`cat`, `dog`, `snake`, `turtle`)\n",
        "- One-hot encoding with **pandas** and with **scikit-learn**\n",
        "\n",
        "> Tip: When using linear models, consider `drop_first=True` (or `drop='first'` in scikit-learn) to avoid perfect multicollinearity.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "csTOFDX3A576",
        "outputId": "76e43a46-ebe6-4ef4-d0ff-c369ce568e7a"
      },
      "outputs": [],
      "source": [
        "# ---------------------------------------------------------\n",
        "# ONE-HOT ENCODING: Stand-alone mini example\n",
        "# ---------------------------------------------------------\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Tiny toy dataset with a categorical column\n",
        "df_animals = pd.DataFrame({\n",
        "    \"id\": [1, 2, 3, 4, 5, 6, 7],\n",
        "    \"species\": [\"cat\", \"dog\", \"snake\", \"cat\", \"dog\", \"turtle\", \"dog\"]\n",
        "})\n",
        "\n",
        "print(\"Original data:\")\n",
        "print(df_animals)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k8xwf_20A4PK",
        "outputId": "40311f68-1ff5-45a1-8ae7-0b5aed61c1ad"
      },
      "outputs": [],
      "source": [
        "# ---------- Option A: Pandas get_dummies ----------\n",
        "# Fast and simple for quick experiments\n",
        "onehot_pandas = pd.get_dummies(df_animals, columns=[\"species\"], prefix=\"is\", dtype=int)\n",
        "print(\"\\nOne-hot encoded with pandas.get_dummies:\")\n",
        "print(onehot_pandas)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fWwwg0XcA4GF",
        "outputId": "69f14dc3-2a95-4f3c-85ed-5a8ecc54326d"
      },
      "outputs": [],
      "source": [
        "# ---------- Option B: scikit-learn OneHotEncoder ----------\n",
        "# Useful inside ML pipelines; can handle train/test consistently\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "ohe = OneHotEncoder(sparse_output=False, handle_unknown=\"ignore\")\n",
        "encoded = ohe.fit_transform(df_animals[[\"species\"]])  # must be 2D\n",
        "\n",
        "encoded_df = pd.DataFrame(encoded, columns=ohe.get_feature_names_out([\"species\"]), index=df_animals.index)\n",
        "onehot_sklearn = pd.concat([df_animals.drop(columns=[\"species\"]), encoded_df], axis=1)\n",
        "\n",
        "print(\"\\nOne-hot encoded with sklearn.OneHotEncoder:\")\n",
        "print(onehot_sklearn)\n",
        "\n",
        "# Notes:\n",
        "# - OneHotEncoder returns a NumPy array; we wrap it into a DataFrame for readability.\n",
        "# - handle_unknown='ignore' is helpful when test data contains a new category.\n",
        "# - For linear models you can use OneHotEncoder(drop='first') to drop one column per category."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GRKCJMDrBkGE"
      },
      "source": [
        "## üìè Optional: Feature Scaling (Why and How)\n",
        "\n",
        "Features often live on **different scales** (e.g., temperature in Kelvin vs. humidity in 0‚Äì1).  \n",
        "Some models (like **k-NN, SVM, linear/logistic regression, neural nets**) are **sensitive to scale** because they use distances or gradients.  \n",
        "Other models (like **decision trees and random forests**) are **mostly insensitive** to scaling.\n",
        "\n",
        "Two common scaling methods:\n",
        "- **Standardization** (`StandardScaler`): center to mean 0 and variance 1  \n",
        "- **Min-Max scaling** (`MinMaxScaler`): squish each feature to a fixed range, usually [0, 1]\n",
        "\n",
        "We‚Äôll use a tiny example with three columns:\n",
        "- `temp_K` (270‚Äì305), `humidity` (0‚Äì1), `day_of_year` (1‚Äì365)\n",
        "and show before/after scaling.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a9Pn0FT8BkvV",
        "outputId": "b7118434-2840-450e-966c-0cc994ff3682"
      },
      "outputs": [],
      "source": [
        "# ---------------------------------------------------------\n",
        "# FEATURE SCALING: Stand-alone mini example\n",
        "# ---------------------------------------------------------\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Tiny toy dataset with very different scales/units\n",
        "df_weather = pd.DataFrame({\n",
        "    \"temp_K\":      [272.0, 289.5, 301.2, 295.0, 280.3],  # Kelvin\n",
        "    \"humidity\":    [0.15,  0.80,  0.40,  0.60,  0.05],   # 0‚Äì1\n",
        "    \"day_of_year\": [  12,   150,   230,   320,    45]    # 1‚Äì365\n",
        "})\n",
        "\n",
        "print(\"Original (unscaled) data:\")\n",
        "print(df_weather)\n",
        "print(\"\\nColumn means (unscaled):\")\n",
        "print(df_weather.mean().round(3))\n",
        "print(\"\\nColumn std dev (unscaled):\")\n",
        "print(df_weather.std(ddof=0).round(3))  # population std for clarity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "63k4y-VqBuc3",
        "outputId": "1ce1027c-1989-453c-a1e1-15a23e2d7f78"
      },
      "outputs": [],
      "source": [
        "# ---------- Option A: Standardization (mean=0, std=1) ----------\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "std_scaler = StandardScaler()\n",
        "weather_std = pd.DataFrame(std_scaler.fit_transform(df_weather),\n",
        "                           columns=df_weather.columns)\n",
        "\n",
        "print(\"\\nStandardized data (‚âà mean 0, std 1):\")\n",
        "print(weather_std.round(3))\n",
        "print(\"\\nMeans after standardization (should be ~0):\")\n",
        "print(weather_std.mean().round(3))\n",
        "print(\"\\nStd dev after standardization (should be ~1):\")\n",
        "print(weather_std.std(ddof=0).round(3))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eXoEFdhIBwr4",
        "outputId": "270a6ab0-8a33-4d36-e788-94ad886ad17d"
      },
      "outputs": [],
      "source": [
        "# ---------- Option B: Min‚ÄìMax scaling to [0, 1] ----------\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "mm_scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "weather_mm = pd.DataFrame(mm_scaler.fit_transform(df_weather),\n",
        "                          columns=df_weather.columns)\n",
        "\n",
        "print(\"\\nMin‚ÄìMax scaled data (range ~[0, 1]):\")\n",
        "print(weather_mm.round(3))\n",
        "print(\"\\nMin per column after Min‚ÄìMax scaling (should be 0):\")\n",
        "print(weather_mm.min().round(3))\n",
        "print(\"\\nMax per column after Min‚ÄìMax scaling (should be 1):\")\n",
        "print(weather_mm.max().round(3))\n",
        "\n",
        "# Notes:\n",
        "# - Always fit scalers on the TRAIN set, then transform both train and test with that fitted scaler.\n",
        "# - Tree-based models (DecisionTree/RandomForest) usually don‚Äôt need scaling.\n",
        "# - Distance/gradient-based models often benefit a lot from scaling."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_bnc6-bKIBvG"
      },
      "source": [
        "# ‚úÇÔ∏è Step 2: Splitting the Data (Training vs Testing)\n",
        "\n",
        "Now that our dataset is clean and explored, it‚Äôs time to prepare it for **machine learning**.  \n",
        "Before we train a model, we should always split the data into **two parts**:\n",
        "\n",
        "- üß© **Training set** ‚Äî the data our model *learns from*  \n",
        "- üß™ **Testing set** ‚Äî the data we *evaluate* the model on later\n",
        "\n",
        "Why do we do this?  \n",
        "Because we want to know how well our model performs on **new, unseen data**, not just the data it has already memorized.\n",
        "\n",
        "> üí° Think of it like studying for an exam:\n",
        "> - One student **understands the concepts** ‚Äî they can solve new problems on test day.  \n",
        "> - Another **memorizes every homework problem** ‚Äî they struggle when the test questions look different.  \n",
        ">\n",
        "> Machine learning works the same way ‚Äî we want our model to **generalize** its understanding, not just memorize.\n",
        "\n",
        "We typically use **80% of the data for training** and **20% for testing**, but this ratio can vary depending on dataset size.  \n",
        "Most importantly, we **shuffle the data** before splitting to ensure both sets are representative of the whole dataset.\n",
        "\n",
        "Let‚Äôs perform this split next.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M-NCoqNEIDTU",
        "outputId": "ddfb22cf-8dbb-4c7e-d5ba-d55f9bb769ae"
      },
      "outputs": [],
      "source": [
        "# =========================================================\n",
        "# ‚úÇÔ∏è STEP 2: Splitting the Data into Training and Testing sets\n",
        "# =========================================================\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# We'll use scikit-learn‚Äôs built-in train_test_split function.\n",
        "# It randomly shuffles and splits the dataset for us.\n",
        "# test_size=0.2 ‚Üí 20% of data for testing, 80% for training\n",
        "# random_state=42 ‚Üí ensures reproducibility (same split each run)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Print the shapes to confirm the split\n",
        "print(\"‚úÖ Data successfully split into training and test sets.\")\n",
        "print(f\"Training set shape: {X_train.shape}\")\n",
        "print(f\"Test set shape:     {X_test.shape}\")\n",
        "\n",
        "# --- Beginner notes ---\n",
        "# - Always keep your test set completely separate; never train on it.\n",
        "# - Shuffling ensures the training and testing data have similar distributions.\n",
        "# - A good rule of thumb: the larger your dataset, the smaller your test split can be.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_m570F09Istg"
      },
      "source": [
        "# üå≥ Step 3: Picking a Model ‚Äî Decision Tree\n",
        "\n",
        "Now that we have our data ready (features split into training and testing), we‚Äôll choose our first machine learning model: a **Decision Tree**.\n",
        "\n",
        "---\n",
        "\n",
        "### üå± What is a Decision Tree?\n",
        "\n",
        "- Imagine you‚Äôre trying to predict house prices, and you ask a series of questions:  \n",
        "  ‚Äúüè† Is the median income here > $70 K?‚Äù ‚Üí Yes / No  \n",
        "  ‚ÄúüèòÔ∏è Is the average number of rooms > 5?‚Äù ‚Üí Yes / No  \n",
        "  Each question splits the data into a smaller group, until you reach a **leaf** that is fairly homogeneous in the target value (median house value).  \n",
        "\n",
        "- In machine-learning terms, the tree:  \n",
        "  üå≥ starts at a **root node** (all data)  \n",
        "  ‚û°Ô∏è makes **decision splits** based on features (internal nodes)  \n",
        "  üçÉ ends in **leaf nodes** where it outputs a prediction (for regression: usually the average target in that leaf)  \n",
        "  [Ref. 1 ‚Äì Analytics Vidhya](https://www.analyticsvidhya.com/blog/2021/08/decision-tree-algorithm/?utm_source=chatgpt.com)  \n",
        "\n",
        "- Because it asks simple **‚Äúif/else‚Äù** questions, it‚Äôs intuitive and easy to explain.\n",
        "\n",
        "---\n",
        "\n",
        "### üí° Why choose a Decision Tree at this stage?\n",
        "\n",
        "- ‚úÖ Handles **numerical input** without needing scaling.  \n",
        "- üîÄ Captures **non-linear relationships** between features and target.  \n",
        "- üëÄ Easy to **visualize and interpret** ‚Äî you can literally draw the tree and trace the decisions.  \n",
        "  [Ref. 2 ‚Äì scikit-learn Example](https://scikit-learn.org/1.5/auto_examples/tree/plot_tree_regression.html?utm_source=chatgpt.com)  \n",
        "- ‚öôÔ∏è Serves as a **strong baseline** before trying more complex models.\n",
        "\n",
        "---\n",
        "\n",
        "### üß† Important to know (before tuning hyperparameters)\n",
        "\n",
        "- The tree splits data where it finds the **largest reduction in error** (for regression, this usually means reducing variance or mean squared error).  \n",
        "  [Ref. 3 ‚Äì Medium Intuition Guide](https://farshadabdulazeez.medium.com/understanding-decision-tree-regressor-an-in-depth-intuition-a1d3af182efd?utm_source=chatgpt.com)  \n",
        "- If it grows **too deep**, it can ‚Äúmemorize‚Äù the training data ‚Äî this is called **overfitting** (we‚Äôll revisit this in the bias-variance section).  \n",
        "  [Ref. 4 ‚Äì Wikipedia: Decision Tree Learning](https://en.wikipedia.org/wiki/Decision_tree_learning?utm_source=chatgpt.com)  \n",
        "- Because it‚Äôs transparent and interpretable, decision trees are perfect for understanding **core ML ideas** before moving to ensembles like Random Forests.\n",
        "\n",
        "---\n",
        "\n",
        "### üî≠ Looking Ahead\n",
        "\n",
        "- Next, we‚Äôll **train** the decision tree on the training data (`X_train`, `y_train`) and test it on unseen data (`X_test`, `y_test`).  \n",
        "- Later, we‚Äôll explore how to **fine-tune** parameters like tree depth and leaf size, and see how that affects the **bias‚Äìvariance tradeoff**.\n",
        "\n",
        "> ‚úÖ **In short:**  \n",
        "> A decision tree is like teaching a model to ask a sequence of ‚Äúyes/no‚Äù questions that gradually narrow down to a prediction.  \n",
        "> It‚Äôs visual, interpretable, and an excellent first step into machine learning models.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OpDYzDE-Ito1",
        "outputId": "6684cf45-23b8-48ce-e947-74006e925500"
      },
      "outputs": [],
      "source": [
        "# =========================================================\n",
        "# Step 3: Picking a Model ‚Äî Decision Tree\n",
        "# =========================================================\n",
        "\n",
        "# We choose a Decision Tree Regressor for our first model.\n",
        "# It will learn from the training data (X_train, y_train)\n",
        "# and we‚Äôll later test how it performs on unseen data (X_test, y_test).\n",
        "\n",
        "dt = DecisionTreeRegressor(random_state=42)  # fix seed for reproducibility\n",
        "\n",
        "# Fit the model on the training set:\n",
        "dt.fit(X_train, y_train)\n",
        "\n",
        "print(\"‚úÖ Decision tree model is trained on the training data.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f3Mt53acLL8P"
      },
      "source": [
        "# üìè Step 4: Evaluating Model Performance\n",
        "\n",
        "Now that we‚Äôve trained our decision tree, it‚Äôs time to measure **how well it performs** ‚Äî both on the data it learned from (*training set*) and on new, unseen data (*test set*).\n",
        "\n",
        "When working with **regression models**, our predictions are *continuous values* (like house prices).  \n",
        "So we can‚Äôt simply count ‚Äúcorrect‚Äù vs ‚Äúincorrect‚Äù predictions ‚Äî instead, we measure *how close* the predictions are to the true values.\n",
        "\n",
        "---\n",
        "\n",
        "### ‚öôÔ∏è Two Key Evaluation Metrics\n",
        "\n",
        "#### üßÆ **1. Root Mean Square Error (RMSE)**\n",
        "\n",
        "RMSE tells us the *average size of our prediction errors*.  \n",
        "If RMSE = 5, it means our predictions are usually off by about 5 (in the same units as the target).  \n",
        "A smaller RMSE means a better model.\n",
        "\n",
        "Mathematically:\n",
        "\n",
        "$$\n",
        "\\text{RMSE}(y_{\\text{true}}, y_{\\text{pred}})\n",
        "= \\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2}\n",
        "$$\n",
        "\n",
        "where  \n",
        "- $y_i$ = actual value  \n",
        "- $\\hat{y}_i$ = predicted value  \n",
        "- $n$ = number of samples  \n",
        "\n",
        "RMSE treats over- and under-predictions as equally bad since the error is squared.\n",
        "\n",
        "---\n",
        "\n",
        "#### üìä **2. R¬≤ Score (Coefficient of Determination)**\n",
        "\n",
        "R¬≤ measures **how much of the variation** in the target variable our model can explain.  \n",
        "It answers the question: *‚ÄúHow well do the model‚Äôs predictions follow the true data pattern?‚Äù*\n",
        "\n",
        "Mathematically:\n",
        "\n",
        "$$\n",
        "R^2(y_{\\text{true}}, y_{\\text{pred}})\n",
        "= 1 - \\frac{\\sum_{i=1}^{n}(y_i - \\hat{y}_i)^2}{\\sum_{i=1}^{n}(y_i - \\bar{y})^2}\n",
        "$$\n",
        "where  \n",
        "$$\n",
        "\\bar{y} = \\frac{1}{n}\\sum_{i=1}^{n} y_i\n",
        "$$\n",
        "\n",
        "Interpretation:\n",
        "- $R^2 = 1.0$ : perfect predictions  \n",
        "- $R^2 = 0.0$ : model does no better than predicting the mean  \n",
        "- $R^2 < 0$ : model performs worse than a flat mean prediction  \n",
        "\n",
        "---\n",
        "\n",
        "### üß† Why We Need Both\n",
        "\n",
        "- **RMSE** gives an *absolute error measure* (in target units).  \n",
        "- **R¬≤** gives a *relative goodness-of-fit measure* (scale-free).  \n",
        "\n",
        "By looking at both, we get a full picture:\n",
        "- RMSE ‚Üí *How far off are we?*  \n",
        "- R¬≤ ‚Üí *How well are we capturing the trend?*  \n",
        "\n",
        "---\n",
        "\n",
        "### üßæ Train vs Test Metrics\n",
        "\n",
        "We always compute metrics on **both** sets:\n",
        "- **Training error** ‚Üí how well the model fits known data.  \n",
        "- **Test error** ‚Üí how well it generalizes to unseen data.  \n",
        "\n",
        "> ‚ö†Ô∏è If training error is low but test error is high, our model has **overfit** (memorized instead of learned).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Af3P1ty7OG8B"
      },
      "source": [
        "## üîÆ Making Predictions with a Trained Model\n",
        "\n",
        "Once a model is trained, we use **`.predict()`** to generate predictions.\n",
        "\n",
        "- On the **training set**: predictions show how well the model learned from the examples it saw.\n",
        "- On the **test set**: predictions show how well the model generalizes to *new, unseen* data.\n",
        "\n",
        "We‚Äôll create:\n",
        "- `y_train_pred` ‚Üí predictions on `X_train`\n",
        "- `y_test_pred`  ‚Üí predictions on `X_test`\n",
        "\n",
        "Then we‚Äôll preview a few predicted vs true values to build intuition.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yrmpjdx3PupE",
        "outputId": "d9604a22-a870-4366-ac07-7c411ff7d723"
      },
      "outputs": [],
      "source": [
        "# -----------------------------------------------------------\n",
        "# üîÆ Predictions from our trained Decision Tree\n",
        "# -----------------------------------------------------------\n",
        "y_train_pred = dt.predict(X_train)\n",
        "y_test_pred  = dt.predict(X_test)\n",
        "\n",
        "# Quick preview (first 5 items) ‚Äî nice aligned table\n",
        "import pandas as pd\n",
        "preview = pd.DataFrame({\n",
        "    \"y_true_test\":  y_test.reset_index(drop=True).head(5),\n",
        "    \"y_pred_test\":  pd.Series(y_test_pred).head(5)\n",
        "})\n",
        "print(\"üîé Test preview (first 5 rows):\")\n",
        "print(preview.to_string(index=False))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rsNoOo8bP7u_"
      },
      "source": [
        "## üìù Activity 2 ‚Äî Compute RMSE (by hand, using your model‚Äôs outputs)\n",
        "\n",
        "**Your task:** Complete the function below so it calculates **RMSE** *without* using scikit-learn.\n",
        "\n",
        "Steps:\n",
        "1) Convert inputs to NumPy arrays  \n",
        "2) Compute squared errors \\((y_{\\text{true}} - y_{\\text{pred}})^2\\)  \n",
        "3) Take the mean of those squared errors  \n",
        "4) Return the square root (that‚Äôs RMSE)\n",
        "\n",
        "> Compute RMSE for both **train** and **test** predictions produced above.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zppTT_K9P80b"
      },
      "outputs": [],
      "source": [
        "# -----------------------------------------------------------\n",
        "# üß© Activity 2 ‚Äî Student Version (fill in the function)\n",
        "# -----------------------------------------------------------\n",
        "import numpy as np\n",
        "\n",
        "def rmse_manual(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    Return the Root Mean Square Error between y_true and y_pred.\n",
        "    Implement this WITHOUT scikit-learn.\n",
        "    \"\"\"\n",
        "    # TODO: your code here\n",
        "    # a = np.array(..., dtype=float)\n",
        "    # b = np.array(..., dtype=float)\n",
        "    # mse = ...\n",
        "    # rmse = ...\n",
        "    # return rmse\n",
        "    raise NotImplementedError(\"Fill in rmse_manual and re-run this cell.\")\n",
        "\n",
        "# Uncomment to test after implementing:\n",
        "# print(f\"Train RMSE (manual): {rmse_manual(y_train, y_train_pred):.3f}\")\n",
        "# print(f\"Test  RMSE (manual): {rmse_manual(y_test,  y_test_pred):.3f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ffY2R_8MQFdm"
      },
      "source": [
        "<details>\n",
        "<summary>‚úÖ Show solution (click to expand)</summary>\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "\n",
        "def rmse_manual(y_true, y_pred):\n",
        "    a = np.array(y_true, dtype=float)\n",
        "    b = np.array(y_pred, dtype=float)\n",
        "    mse = np.mean((a - b) ** 2)\n",
        "    return np.sqrt(mse)\n",
        "\n",
        "print(f\"Train RMSE (manual): {rmse_manual(y_train, y_train_pred):.3f}\")\n",
        "print(f\"Test  RMSE (manual): {rmse_manual(y_test,  y_test_pred):.3f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g_QnO1QUQZ9y"
      },
      "source": [
        "## üì¶ Using scikit-learn for RMSE and R¬≤\n",
        "\n",
        "- **RMSE** via `mean_squared_error(y_true, y_pred)` ‚Üí take the **square root**  \n",
        "- **R¬≤** via `r2_score(y_true, y_pred)` ‚Üí **order matters**: pass **true first**, then **predicted**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lmjPxfdBQa7p",
        "outputId": "5dc5ec87-5fa4-466c-ab4d-8a591aa7adc5"
      },
      "outputs": [],
      "source": [
        "# =========================================================\n",
        "# üìè STEP 4: Evaluation Metrics (RMSE and R¬≤) ‚Äî scikit-learn\n",
        "# =========================================================\n",
        "import numpy as np\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "# Compute Root Mean Square Error (RMSE)\n",
        "rmse_train = np.sqrt(mean_squared_error(y_train, y_train_pred))\n",
        "rmse_test  = np.sqrt(mean_squared_error(y_test,  y_test_pred))\n",
        "\n",
        "# Compute R¬≤ Score (Coefficient of Determination)\n",
        "# ‚ö†Ô∏è Order matters: r2_score(y_true, y_pred)\n",
        "r2_train = r2_score(y_train, y_train_pred)\n",
        "r2_test  = r2_score(y_test,  y_test_pred)\n",
        "\n",
        "# Display the results neatly\n",
        "print(\"‚úÖ Model Evaluation Results\")\n",
        "print(\"=============================\")\n",
        "print(f\"| Train RMSE : {rmse_train:.3f}\")\n",
        "print(f\"| Test  RMSE : {rmse_test:.3f}\")\n",
        "print(\"-----------------------------\")\n",
        "print(f\"| Train R¬≤   : {r2_train:.3f}\")\n",
        "print(f\"| Test  R¬≤   : {r2_test:.3f}\")\n",
        "\n",
        "# Beginner notes:\n",
        "# - RMSE is in the same units as the target (easy to interpret).\n",
        "# - R¬≤ is unitless and shows how much variance is explained.\n",
        "# - Big train‚Äìtest gap ‚Üí likely overfitting."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1mY78MG_R3pz"
      },
      "source": [
        "# üåø Step 5: Hyperparameter Tuning\n",
        "\n",
        "A **hyperparameter** is a setting you choose *before* training that controls **how the model learns**.  \n",
        "It‚Äôs not ‚Äúlearned‚Äù from the data ‚Äî you pick it manually or through tuning.\n",
        "\n",
        "For a **Decision Tree**, some key hyperparameters are:\n",
        "\n",
        "- `max_depth` ‚Üí limits how many ‚Äúlevels‚Äù the tree can grow  \n",
        "- `min_samples_split` ‚Üí minimum samples required to split a node  \n",
        "- `min_samples_leaf` ‚Üí minimum samples required in any leaf  \n",
        "- `criterion` ‚Üí how the tree measures ‚Äúerror‚Äù to decide splits  \n",
        "\n",
        "---\n",
        "\n",
        "### üå± What is `max_depth`?\n",
        "\n",
        "`max_depth` decides **how many questions** the tree can ask before making a prediction.\n",
        "\n",
        "- A **smaller** `max_depth` ‚Üí fewer questions ‚Üí simpler, shorter tree  \n",
        "- A **larger** `max_depth` ‚Üí more questions ‚Üí detailed, larger tree  \n",
        "\n",
        "If the tree is too shallow, it might **miss important patterns**.  \n",
        "If it‚Äôs too deep, it might **memorize every small detail** (we‚Äôll talk about this later!).\n",
        "\n",
        "You can read more here:  \n",
        "üëâ [DecisionTreeRegressor Docs](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a_g636X-R4Z-",
        "outputId": "c54e5935-97fb-4074-f634-7824399d5757"
      },
      "outputs": [],
      "source": [
        "# =========================================================\n",
        "# üåø STEP 5: Exploring max_depth interactively\n",
        "# =========================================================\n",
        "# Prompt the learner for a depth\n",
        "raw = input(\"Enter a value for max_depth (e.g., 3, 5, 10, or 'None' for unlimited): \").strip()\n",
        "\n",
        "if raw.lower() == \"none\" or raw == \"\":\n",
        "    max_depth_value = None\n",
        "else:\n",
        "    try:\n",
        "        max_depth_value = int(raw)\n",
        "    except ValueError:\n",
        "        print(\"‚ö†Ô∏è Invalid input. Using default value = 5\")\n",
        "        max_depth_value = 5\n",
        "\n",
        "# Train a new tree with the chosen depth\n",
        "dt_manual = DecisionTreeRegressor(max_depth=max_depth_value, random_state=42)\n",
        "dt_manual.fit(X_train, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_train_pred = dt_manual.predict(X_train)\n",
        "y_test_pred  = dt_manual.predict(X_test)\n",
        "\n",
        "# Evaluation metrics\n",
        "rmse_train = np.sqrt(mean_squared_error(y_train, y_train_pred))\n",
        "rmse_test  = np.sqrt(mean_squared_error(y_test,  y_test_pred))\n",
        "r2_train   = r2_score(y_train, y_train_pred)\n",
        "r2_test    = r2_score(y_test,  y_test_pred)\n",
        "\n",
        "depth_label = \"None (unlimited)\" if max_depth_value is None else str(max_depth_value)\n",
        "\n",
        "print(\"\\n================ Decision Tree Performance ================\")\n",
        "print(f\"| max_depth : {depth_label}\")\n",
        "print(\"|----------------------------------------------------------\")\n",
        "print(f\"| Train RMSE : {rmse_train:.3f}   | Test RMSE : {rmse_test:.3f}\")\n",
        "print(f\"| Train R¬≤   : {r2_train:.3f}     | Test R¬≤   : {r2_test:.3f}\")\n",
        "print(\"============================================================\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bpBhPUJUSVwv"
      },
      "source": [
        "---\n",
        "\n",
        "### üí¨ Activity 3 ‚Äî Which `max_depth` is the best?\n",
        "\n",
        "Try running the cell above several times with different values  \n",
        "(for example: 2, 4, 6, 10, None).  \n",
        "Then discuss with your group or note down your thoughts.\n",
        "\n",
        "**Questions to consider:**\n",
        "1. Which value of `max_depth` gave the best results overall?  \n",
        "2. How do the *training* and *testing* metrics compare as `max_depth` increases?  \n",
        "3. Why do you think performance changes when we make the tree deeper or shallower?\n",
        "\n",
        "<details>\n",
        "<summary>üí° Click to view hints</summary>\n",
        "\n",
        "- Look for a depth where **both train and test RMSE** are reasonably low ‚Äî not just train!  \n",
        "- If train results are excellent but test results get worse, the model might be relying too much on specific details in the training data.  \n",
        "- If both train and test errors are high, the model may be too simple to capture patterns.  \n",
        "- There‚Äôs usually a **‚Äúsweet spot‚Äù** where the model learns just enough structure to make good predictions on new data.\n",
        "\n",
        "</details>\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y33plQgZVB9n"
      },
      "source": [
        "## üß≠ Revisiting Data Splitting ‚Äî Validation Split & Data Leakage\n",
        "\n",
        "So far we‚Äôve trained models using just **training** and **testing** data.  Now we introduce a third split: the **validation set**.\n",
        "\n",
        "### üß© Why do we need a validation set?\n",
        "When tuning hyperparameters (like `max_depth`), we often test many versions of the same model.  \n",
        "If we used the **test set** each time, we‚Äôd be secretly teaching the model about that data ‚Äî this is called **data leakage**.\n",
        "\n",
        "A **validation set** solves this:\n",
        "- The **training set** teaches the model.\n",
        "- The **validation set** helps us *choose* the best hyperparameters.\n",
        "- The **test set** is kept aside until the very end for *final evaluation*.\n",
        "\n",
        "> üí° Think of it like studying for an exam:  \n",
        "> - Training = practice questions  \n",
        "> - Validation = mock test to adjust your strategy  \n",
        "> - Test = the real exam you haven‚Äôt seen before\n",
        "\n",
        "Below, we‚Äôll split the training data again to create a small validation set and test how model performance changes as we vary `max_depth`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mc5_BEnlVbOK",
        "outputId": "76e5c264-f90c-48e4-ecf6-2118e450cf7c"
      },
      "outputs": [],
      "source": [
        "# =========================================================\n",
        "# üß≠ STEP 6: Creating a Validation Set and Running Experiments\n",
        "# =========================================================\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "\n",
        "# Split the *training* data again: 75% for actual training, 25% for validation\n",
        "X_train_sub, X_valid, y_train_sub, y_valid = train_test_split(\n",
        "    X_train, y_train, test_size=0.25, random_state=42\n",
        ")\n",
        "\n",
        "# Prepare lists to store RMSE values for each depth\n",
        "train_rmse, valid_rmse = [], []\n",
        "depths = range(1, 16)\n",
        "\n",
        "for d in depths:\n",
        "    model = DecisionTreeRegressor(max_depth=d, random_state=42)\n",
        "    model.fit(X_train_sub, y_train_sub)\n",
        "\n",
        "    # Predictions\n",
        "    y_train_pred = model.predict(X_train_sub)\n",
        "    y_valid_pred = model.predict(X_valid)\n",
        "\n",
        "    # RMSE\n",
        "    train_rmse.append(np.sqrt(mean_squared_error(y_train_sub, y_train_pred)))\n",
        "    valid_rmse.append(np.sqrt(mean_squared_error(y_valid, y_valid_pred)))\n",
        "\n",
        "print(\"‚úÖ Finished evaluating models across depths 1‚Äì15.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mDSgAYXbVgO9"
      },
      "source": [
        "# ‚öñÔ∏è Step 6: Bias‚ÄìVariance Tradeoff ‚Äî Understanding Model Complexity\n",
        "\n",
        "Now that we have training and validation errors for each `max_depth`,  \n",
        "we can visualize the classic **Bias‚ÄìVariance Tradeoff**.\n",
        "\n",
        "### üå± What does ‚ÄúBias‚ÄìVariance Tradeoff‚Äù mean?\n",
        "\n",
        "When a model is **too simple**, it cannot capture the real patterns in data.  \n",
        "This is called **underfitting** ‚Äî the model has **high bias**.\n",
        "\n",
        "When a model is **too complex**, it memorizes the training data and performs poorly on new data.  \n",
        "This is called **overfitting** ‚Äî the model has **high variance**.\n",
        "\n",
        "| Situation | Description | Typical Metrics |\n",
        "|------------|--------------|-----------------|\n",
        "| **Underfit (high bias)** | Model is too simple | High train & validation errors |\n",
        "| **Overfit (high variance)** | Model too detailed | Low train error, high validation error |\n",
        "| **Good fit** | Just right | Both errors low and close together |\n",
        "\n",
        "The `max_depth` hyperparameter controls **model complexity**.  \n",
        "- Small depth ‚Üí simpler tree ‚Üí underfit region  \n",
        "- Large depth ‚Üí complex tree ‚Üí overfit region  \n",
        "\n",
        "We‚Äôll plot both **train** and **validation** RMSE across depths to see this curve.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 493
        },
        "id": "Qs7OAEfVVhsT",
        "outputId": "b0309c6b-69a6-4e62-c07b-7e957e6ba38e"
      },
      "outputs": [],
      "source": [
        "# =========================================================\n",
        "# ‚öñÔ∏è STEP 7: Bias‚ÄìVariance Tradeoff Plot\n",
        "# =========================================================\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "plt.figure(figsize=(8, 5))\n",
        "sns.lineplot(x=depths, y=train_rmse, label='Train RMSE', marker='o')\n",
        "sns.lineplot(x=depths, y=valid_rmse, label='Validation RMSE', marker='s')\n",
        "plt.title('Bias‚ÄìVariance Tradeoff: Effect of Tree Depth', fontsize=14)\n",
        "plt.xlabel('Tree Depth (Model Complexity)')\n",
        "plt.ylabel('RMSE')\n",
        "plt.xticks(ticks=list(depths))  # remove decimal x-ticks\n",
        "plt.legend()\n",
        "plt.grid(alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G-GQkUKmXKsn"
      },
      "source": [
        "### üí¨ Activity 4 ‚Äî Identify the Best `max_depth`\n",
        "\n",
        "Look at the curve above.\n",
        "\n",
        "**Questions:**\n",
        "1. At which depth does the **validation RMSE** reach its minimum?\n",
        "2. Does the training RMSE keep decreasing after that point?\n",
        "3. Why might validation error increase again for larger depths?\n",
        "\n",
        "Now, using that ‚Äúbest‚Äù `max_depth`, retrain the model on the **entire training set** (`X_train`, `y_train`)  \n",
        "and finally evaluate it on the **test set** (`X_test`, `y_test`).\n",
        "\n",
        "<details>\n",
        "<summary>üí° Click for hints</summary>\n",
        "\n",
        "- You‚Äôre looking for the **lowest point** on the *validation RMSE* curve.  \n",
        "- After that depth, the training error keeps dropping, but validation error rises ‚Üí possible overfitting.  \n",
        "- This ‚Äúturning point‚Äù is your best depth ‚Äî it balances learning the signal vs memorizing noise.  \n",
        "- Retrain once with that depth on all training data (no validation split) and check how it performs on the test set.\n",
        "\n",
        "</details>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X01tZ_jjXTBy",
        "outputId": "e6faf7d6-97a5-440d-d89c-387f8c7b296a"
      },
      "outputs": [],
      "source": [
        "# -----------------------------------------------------------\n",
        "# üß© Activity 4 ‚Äî Retrain with Best Depth and Evaluate on Test\n",
        "# -----------------------------------------------------------\n",
        "best_depth = int(input(\"Enter the best max_depth you observed from the plot: \"))\n",
        "\n",
        "final_tree = DecisionTreeRegressor(max_depth=best_depth, random_state=42)\n",
        "final_tree.fit(X_train, y_train)\n",
        "\n",
        "y_train_pred = final_tree.predict(X_train)\n",
        "y_test_pred  = final_tree.predict(X_test)\n",
        "\n",
        "rmse_train = np.sqrt(mean_squared_error(y_train, y_train_pred))\n",
        "rmse_test  = np.sqrt(mean_squared_error(y_test,  y_test_pred))\n",
        "r2_train   = r2_score(y_train, y_train_pred)\n",
        "r2_test    = r2_score(y_test,  y_test_pred)\n",
        "\n",
        "print(\"\\n================ Final Model Evaluation ================\")\n",
        "print(f\"| Chosen max_depth : {best_depth}\")\n",
        "print(\"|-------------------------------------------------------\")\n",
        "print(f\"| Train RMSE : {rmse_train:.3f}   | Test RMSE : {rmse_test:.3f}\")\n",
        "print(f\"| Train R¬≤   : {r2_train:.3f}     | Test R¬≤   : {r2_test:.3f}\")\n",
        "print(\"========================================================\")\n",
        "print(\"üéØ See if your model generalizes well ‚Äî are train and test metrics close?\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WorZtkm6GKcT"
      },
      "source": [
        "# üå≤ Bonus Section: Reducing Variance in Machine Learning Models\n",
        "\n",
        "By now, you‚Äôve seen how decision trees can easily **overfit** if they grow too deep.  \n",
        "This happens because most ML models are **powerful enough to memorize** patterns in the training data,  \n",
        "which makes them perform worse on unseen data.\n",
        "\n",
        "---\n",
        "\n",
        "## ü§î Why Do Models Overfit?\n",
        "\n",
        "When a model is very flexible (like a deep tree or a large neural network),  \n",
        "it can perfectly match all points in the training set ‚Äî even the noise!  \n",
        "That means it performs well on training data but struggles on validation or test data.\n",
        "\n",
        "So how can we **reduce variance** and make models more stable?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DWzS7oGnf2Gk"
      },
      "source": [
        "## üß© Introducing Ensembling\n",
        "\n",
        "**Ensembling** means combining the predictions of many smaller models to create a stronger overall model.  \n",
        "The idea is simple: **many weak learners together make a strong learner.**\n",
        "\n",
        "One of the most popular ensemble methods is the **Random Forest**.\n",
        "\n",
        "### üå≥ Random Forests in simple terms:\n",
        "- Build **many decision trees**, each trained on a random subset of the data and features.  \n",
        "- Each tree makes a prediction, and the **forest averages their results**.  \n",
        "- Randomness ensures each tree learns something slightly different.  \n",
        "- The average prediction is smoother, less noisy, and less likely to overfit.\n",
        "\n",
        "> ü™Ñ Think of each tree as a student:  \n",
        "> One tree may make mistakes, but if we average answers from 50 independent students,  \n",
        "> we‚Äôll likely get a more reliable overall answer.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KW4r94hpf4Ne"
      },
      "source": [
        "## ‚öôÔ∏è Step 1 ‚Äî Training Multiple Random Forests\n",
        "\n",
        "We‚Äôll tune two important hyperparameters:\n",
        "- `n_estimators`: number of trees in the forest üå≤  \n",
        "- `max_depth`: maximum depth of each tree üåø\n",
        "\n",
        "We‚Äôll run a small **grid search** over combinations of these values  \n",
        "to find which pair gives the **lowest validation RMSE**.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 610
        },
        "id": "2hPukg6HYO3g",
        "outputId": "27bb0079-143f-4659-b249-c0e0a3e19dc5"
      },
      "outputs": [],
      "source": [
        "# =========================================================\n",
        "# üå≤ STEP 1: Random Forest Grid Search (tuning hyperparameters)\n",
        "# =========================================================\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "results = []\n",
        "\n",
        "for n in [4, 8, 16, 32, 64]:           # number of trees\n",
        "    for d in [2, 4, 8, 16]:            # depth of each tree\n",
        "        rf = RandomForestRegressor(\n",
        "            n_estimators=n,\n",
        "            max_depth=d,\n",
        "            random_state=42,\n",
        "            n_jobs=-1\n",
        "        )\n",
        "        rf.fit(X_train_sub, y_train_sub)\n",
        "        y_valid_pred = rf.predict(X_valid)\n",
        "        rmse_valid = np.sqrt(mean_squared_error(y_valid, y_valid_pred))\n",
        "        results.append({'n_estimators': n, 'max_depth': d, 'val_RMSE': rmse_valid})\n",
        "\n",
        "rf_df = pd.DataFrame(results)\n",
        "\n",
        "# Pivot results for visualization\n",
        "rf_pivot = rf_df.pivot(index='max_depth', columns='n_estimators', values='val_RMSE')\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(rf_pivot, annot=True, cmap='crest', fmt=\".3f\")\n",
        "plt.title('Validation RMSE for Random Forest (Grid Search)', fontsize=14)\n",
        "plt.xlabel('Number of Trees (n_estimators)')\n",
        "plt.ylabel('Tree Depth (max_depth)')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"‚úÖ Grid search complete ‚Äî check the heatmap for the best combination!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QH71oonSYSNK"
      },
      "source": [
        "---\n",
        "\n",
        "### üí¨ Activity 5 ‚Äî Discussion: Interpreting the Heatmap\n",
        "\n",
        "Take a moment to examine the **validation RMSE heatmap**.\n",
        "\n",
        "**Questions:**\n",
        "1. What happens to the RMSE as we increase the number of trees (`n_estimators`)?  \n",
        "2. What happens as we increase the depth (`max_depth`)?  \n",
        "3. Which combination of these hyperparameters gives the *lowest* validation error?\n",
        "\n",
        "<details>\n",
        "<summary>üí° Click for hints</summary>\n",
        "\n",
        "- More trees generally make the model more stable (but take longer to train).  \n",
        "- Deeper trees can capture more detail, but too much depth can lead to overfitting ‚Äî just like before!  \n",
        "- You‚Äôre looking for the smallest number in the heatmap (lowest validation RMSE).  \n",
        "  That combination represents the most balanced model.\n",
        "</details>\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KeafZmaZYcM1"
      },
      "source": [
        "## üèÜ Step 2 ‚Äî Evaluating the Best Model on the Test Set\n",
        "\n",
        "Now that we‚Äôve found the best hyperparameters from the validation results,  \n",
        "let‚Äôs retrain a final Random Forest using those values on the **entire training data**  \n",
        "and then test it on the **unseen test set**.[link text](https://)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jqw8XaxmYc_3",
        "outputId": "fbdc53fa-0776-4964-826f-872c8055204d"
      },
      "outputs": [],
      "source": [
        "# =========================================================\n",
        "# üèÜ STEP 2: Final Evaluation on Test Data\n",
        "# =========================================================\n",
        "best_params = rf_df.loc[rf_df['val_RMSE'].idxmin()]\n",
        "print(f\"Best hyperparameters: {best_params.to_dict()}\")\n",
        "\n",
        "rf_best = RandomForestRegressor(\n",
        "    n_estimators=int(best_params['n_estimators']),\n",
        "    max_depth=int(best_params['max_depth']),\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "rf_best.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate on test data\n",
        "y_test_pred = rf_best.predict(X_test)\n",
        "rf_rmse_test = np.sqrt(mean_squared_error(y_test, y_test_pred))\n",
        "rf_r2_test = r2_score(y_test, y_test_pred)\n",
        "\n",
        "print(\"\\nüéØ Final Random Forest Performance on Test Data\")\n",
        "print(\"=================================================\")\n",
        "print(f\"| RMSE : {rf_rmse_test:.3f}\")\n",
        "print(f\"| R¬≤   : {rf_r2_test:.3f}\")\n",
        "print(\"=================================================\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Vybatf6Y48m"
      },
      "source": [
        "---\n",
        "\n",
        "### üí¨ Discussion ‚Äî Comparing Random Forest vs Tuned Decision Tree\n",
        "\n",
        "Now that we‚Äôve evaluated the **Random Forest** on the test set,  \n",
        "let‚Äôs compare its performance to the **best Decision Tree** model you found earlier (from Activity 4).\n",
        "\n",
        "**Questions:**\n",
        "1. Are the Random Forest‚Äôs test RMSE and R¬≤ better than those of the tuned Decision Tree?  \n",
        "2. If yes ‚Äî *why* do you think that happened?  \n",
        "3. What trade-offs might we face when using a Random Forest instead of a single Decision Tree?\n",
        "\n",
        "<details>\n",
        "<summary>üí° Click to view hints</summary>\n",
        "\n",
        "- Random Forests combine the outputs of many trees, so their predictions are **more stable** and less affected by noise.  \n",
        "- This **reduces variance**, which often leads to **lower RMSE** and **higher R¬≤** on unseen data.  \n",
        "- However, because they average over many trees, Random Forests lose some of the **interpretability** that makes single trees so easy to understand.  \n",
        "- In general:  \n",
        "  - üéØ **Decision Tree:** simple, interpretable, but prone to overfitting.  \n",
        "  - üå≥ **Random Forest:** complex, harder to visualize, but usually more accurate and robust.\n",
        "</details>\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "haP_0vyrY6Rb"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZCQd6azjdY4V"
      },
      "source": [
        "# üéì Conclusion ‚Äî Putting It All Together\n",
        "\n",
        "Congratulations! You‚Äôve now completed a full walkthrough of how to **evaluate, tune, and interpret** a machine learning model from start to finish.\n",
        "\n",
        "Let‚Äôs recap what we accomplished:\n",
        "\n",
        "---\n",
        "\n",
        "## ‚úÖ What We Did\n",
        "\n",
        "1. **Prepared and inspected data**  \n",
        "   - Explored the California Housing dataset and visualized its features.  \n",
        "   - Discussed what each feature represents and how it might influence the target.\n",
        "\n",
        "2. **Split the data into training and testing sets**  \n",
        "   - Understood why separate data is needed to fairly measure performance.\n",
        "\n",
        "3. **Trained our first model ‚Äî Decision Tree**  \n",
        "   - Learned how models use data to make predictions.  \n",
        "   - Introduced the idea of *hyperparameters* and how they control learning.\n",
        "\n",
        "4. **Evaluated performance using RMSE and R¬≤**  \n",
        "   - Understood what these metrics mean and how to interpret their values.  \n",
        "   - Practiced computing RMSE manually for intuition.\n",
        "\n",
        "5. **Tuned hyperparameters and introduced the validation split**  \n",
        "   - Observed how changing `max_depth` affects performance.  \n",
        "   - Learned why using a validation set helps us choose fair hyperparameters.\n",
        "\n",
        "6. **Explored the Bias‚ÄìVariance Tradeoff**  \n",
        "   - Visualized how model complexity influences training and validation errors.  \n",
        "   - Defined *underfitting* (high bias) and *overfitting* (high variance).  \n",
        "   - Identified the ‚Äúsweet spot‚Äù where the model generalizes best.\n",
        "\n",
        "7. **Reduced variance using Random Forests**  \n",
        "   - Saw how combining many trees makes predictions more stable.  \n",
        "   - Compared results against our tuned Decision Tree and discussed why ensembles often perform better.\n",
        "\n",
        "---\n",
        "\n",
        "## üß† Key Takeaways\n",
        "\n",
        "- **Model evaluation** is not just about metrics ‚Äî it‚Äôs about understanding *why* a model performs the way it does.  \n",
        "- **Hyperparameter tuning** is about balancing learning power with generalization.  \n",
        "- **Bias‚Äìvariance tradeoff** helps us reason about model behavior across complexity levels.  \n",
        "- **Ensembles** are a practical way to reduce variance and improve stability.  \n",
        "- These same ideas apply to *any* ML or DL model ‚Äî from simple regressions to neural networks.\n",
        "\n",
        "---\n",
        "\n",
        "## üí¨ Final Discussion ‚Äî Applying ML to Your Own Domain\n",
        "\n",
        "Now it‚Äôs your turn!  \n",
        "Think about a problem in your **own field or area of interest** where you could apply what you‚Äôve learned.\n",
        "\n",
        "**Question:**  \n",
        "> Can you describe one problem where machine learning might be useful?  \n",
        "\n",
        "In your answer, include:\n",
        "1. üéØ **Goal:** What are you trying to predict or classify?  \n",
        "2. üß© **Input features:** What data or variables would you use as inputs?  \n",
        "3. üéØ **Output:** Is it a continuous number (regression) or a category (classification)?  \n",
        "4. üìä **Data collection:** Where could the data come from? Is it already available or needs to be created?  \n",
        "5. ‚öñÔ∏è **Evaluation:** How would you measure if your model is performing well?  \n",
        "6. üí° **Next steps:** How could you improve the model once you have a baseline?\n",
        "\n",
        "Take a few minutes to think and discuss in your group.  \n",
        "Be as specific as possible ‚Äî the goal is to start **connecting machine learning to your own research or real-world problems.**\n",
        "\n",
        "---\n",
        "\n",
        "> üèÅ **End of Notebook:**  \n",
        "> You‚Äôve now built, evaluated, and improved real ML models ‚Äî and more importantly,  \n",
        "> you‚Äôve learned a structured way to think about *how* and *why* they work.\n",
        ">\n",
        "> üå± The next step? Try applying this same process to your own dataset or domain problem.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hcla2yKYda7Y"
      },
      "source": [
        "---\n",
        "\n",
        "# üìö References & Further Reading\n",
        "\n",
        "If you‚Äôd like to explore these ideas in more depth, here are some excellent beginner-friendly resources that inspired parts of this notebook:\n",
        "\n",
        "### üß© General ML Workflow\n",
        "- [Preparing data for a machine learning model ‚Äî Jeremy Jordan](https://www.jeremyjordan.me/preparing-data-for-a-machine-learning-model/)  \n",
        "  A practical guide on data preprocessing and feature selection.\n",
        "- [Evaluating a machine learning model ‚Äî Jeremy Jordan](https://www.jeremyjordan.me/evaluating-a-machine-learning-model/)  \n",
        "  Explains key evaluation concepts such as overfitting, validation, and test performance.\n",
        "\n",
        "### ‚öôÔ∏è Hyperparameter Tuning & Validation\n",
        "- [Hyperparameter tuning ‚Äî Jeremy Jordan](https://www.jeremyjordan.me/hyperparameter-tuning/)  \n",
        "  Clear explanation of why tuning matters and how to do it responsibly.\n",
        "- [Better validation and test strategies ‚Äî Chang-Hsin Lee](https://changhsinlee.com/better-validation-test/)  \n",
        "  Discusses data leakage, proper validation splits, and fair model comparison.\n",
        "- [Model validation techniques explained ‚Äî Towards Data Science](https://towardsdatascience.com/model-validation-techniques-explained-a-visual-guide-with-code-examples-eb13bbdc8f88/)  \n",
        "  Visual guide to different validation strategies such as hold-out, K-fold, and cross-validation.\n",
        "\n",
        "### üå≥ Feature Engineering\n",
        "- [Feature Engineering Course ‚Äî Kaggle Learn](https://www.kaggle.com/learn/feature-engineering)  \n",
        "  Hands-on introduction to encoding, scaling, and transforming features for better model performance.\n",
        "\n",
        "### ‚öñÔ∏è Bias‚ÄìVariance & Model Interpretation\n",
        "- [Bias‚ÄìVariance Tradeoff Explained ‚Äî Towards Data Science](https://towardsdatascience.com/bias-variance-tradeoff-explained-a-visual-guide-with-code-examples-for-beginners-9521871f728a/)  \n",
        "  Excellent visual explanation of underfitting, overfitting, and model complexity."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
